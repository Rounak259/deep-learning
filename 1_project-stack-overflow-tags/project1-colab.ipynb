{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-mq46VmhjVb7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PSH06RPsjwpt"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_file_path = \"/content/stacksample.zip\"\n",
        "extract_to = \"/content/stacksample\"\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JrQMYVC9jXho"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n9LPQ_j4GX1O"
      },
      "outputs": [],
      "source": [
        "tags = pd.read_csv('/content/stacksample/Tags.csv', encoding='ISO-8859-1')\n",
        "questions = pd.read_csv(r'/content/stacksample/Questions.csv', encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7Uy1z7t3MJ1Q"
      },
      "outputs": [],
      "source": [
        "tags = tags.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC4EoQB0jc6K",
        "outputId": "b71d9ee1-fe07-456b-ded8-012277a2b28b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m21275/21275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 24ms/step - accuracy: 0.5274 - loss: 0.1276 - val_accuracy: 0.7299 - val_loss: 0.0711\n",
            "Epoch 2/5\n",
            "\u001b[1m21275/21275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 24ms/step - accuracy: 0.7373 - loss: 0.0679 - val_accuracy: 0.7387 - val_loss: 0.0685\n",
            "Epoch 3/5\n",
            "\u001b[1m21275/21275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 26ms/step - accuracy: 0.7497 - loss: 0.0635 - val_accuracy: 0.7389 - val_loss: 0.0676\n",
            "Epoch 4/5\n",
            "\u001b[1m21275/21275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 24ms/step - accuracy: 0.7582 - loss: 0.0607 - val_accuracy: 0.7401 - val_loss: 0.0681\n",
            "Epoch 5/5\n",
            "\u001b[1m21275/21275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 25ms/step - accuracy: 0.7655 - loss: 0.0584 - val_accuracy: 0.7366 - val_loss: 0.0690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Map Tags to Questions using 'Id'\n",
        "questions['Tag'] = questions['Id'].map(tags.groupby('Id')['Tag'].apply(list))\n",
        "questions['Tag'] = questions['Tag'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "questions = questions.dropna(subset=['Tag'])  # Drop rows with no tags\n",
        "\n",
        "# top 20 common tags only\n",
        "from collections import Counter\n",
        "tag_counter = Counter([tag for tags in questions['Tag'] for tag in tags])\n",
        "top_tags = [tag for tag, _ in tag_counter.most_common(20)]\n",
        "questions['Tag'] = questions['Tag'].apply(lambda tags: [tag for tag in tags if tag in top_tags])\n",
        "questions = questions[questions['Tag'].apply(len) > 0]\n",
        "\n",
        "# Clean Questions\n",
        "questions['Body'] = questions['Body'].apply(clean_text)\n",
        "\n",
        "# Encode Tags\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(questions['Tag'])\n",
        "\n",
        "# Tokenize and Pad Sequences\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(questions['Body'])\n",
        "X = tokenizer.texts_to_sequences(questions['Body'])\n",
        "X = pad_sequences(X, maxlen=500)\n",
        "\n",
        "# Split Data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128, input_length=500),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(y.shape[1], activation='sigmoid')  # Multi-label output\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
        "model.save('/content/multilabel_model.h5')\n",
        "\n",
        "# Save Tokenizer and MultiLabelBinarizer\n",
        "import pickle\n",
        "with open('/content/tokenizer.pkl', 'wb') as token_file:\n",
        "    pickle.dump(tokenizer, token_file)\n",
        "with open('/content/mlb.pkl', 'wb') as mlb_file:\n",
        "    pickle.dump(mlb, mlb_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0s9sGKKn76f"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pandas\n",
        "# !pip install numpy\n",
        "# !pip install tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using the model for predicting tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKiNDr2rgEBn",
        "outputId": "8bf2392f-4a0d-4cfb-ee64-f94e02b332bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['javascript', 'java', 'c#', 'php', 'android', 'jquery', 'python', 'html', 'c++', 'ios', 'mysql', 'css', 'sql', 'asp.net', 'objective-c', 'ruby-on-rails', '.net', 'c', 'iphone', 'angularjs']\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "print(top_tags)\n",
        "print(len(top_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FHZVe61oisjk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QYiF6odriwhn"
      },
      "outputs": [],
      "source": [
        "with open('/content/tokenizer.pkl', 'rb') as token_file:\n",
        "    tokenizer = pickle.load(token_file)\n",
        "with open('/content/mlb.pkl', 'rb') as mlb_file:\n",
        "    mlb = pickle.load(mlb_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWYFSfeXi29V",
        "outputId": "7562ce11-7149-4265-dc9e-229c5caa245c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model = load_model('multilabel_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yo4pdOFIi3hT"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = text.lower()\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=500)\n",
        "    return padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wGqp80zoi5Ih"
      },
      "outputs": [],
      "source": [
        "def predict_tags(text, threshold=0.5):\n",
        "    processed_text = preprocess_text(text)\n",
        "    predictions = model.predict(processed_text)\n",
        "    predicted_tags = (predictions > threshold).astype(int)\n",
        "    return mlb.inverse_transform(predicted_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmCISsRtgzwa",
        "outputId": "86c03fa5-c465-4772-adb2-9b93ad7ed165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Predicted Tags: [('c++',)]\n"
          ]
        }
      ],
      "source": [
        "new_question = \"How would you implement a smart pointer to manage memory dynamically?\"\n",
        "tags = predict_tags(new_question)\n",
        "print(\"Predicted Tags:\", tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADc2cyLSgr44",
        "outputId": "5741cd70-3e53-4536-f3cb-ff25b15c1176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "Predicted Tags: [('python',)]\n"
          ]
        }
      ],
      "source": [
        "new_question = \"How would you implement a decorator to measure the execution time of any function it wraps?\"\n",
        "tags = predict_tags(new_question)\n",
        "print(\"Predicted Tags:\", tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuHoExg4i6eB",
        "outputId": "cb2bf73b-c545-46ab-bc86-c7544092e642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "Predicted Tags: [('python',)]\n"
          ]
        }
      ],
      "source": [
        "new_question = \"What is PIP?\"\n",
        "tags = predict_tags(new_question)\n",
        "print(\"Predicted Tags:\", tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBLe3T7Af5jq",
        "outputId": "612f251b-7d6b-4856-a8c8-078e805ba574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Predicted Tags: [('sql',)]\n"
          ]
        }
      ],
      "source": [
        "new_question = \"How would you write a query to retrieve the top N records from a table based on a specific column value\"\n",
        "tags = predict_tags(new_question)\n",
        "print(\"Predicted Tags:\", tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGYJTBzBjLG-",
        "outputId": "097ec686-ad61-4504-8fb1-5a97df20b8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "Predicted Tags: [('sql',)]\n"
          ]
        }
      ],
      "source": [
        "new_question = \"How would you secure form submissions and prevent SQL injection vulnerabilities?\"\n",
        "tags = predict_tags(new_question)\n",
        "print(\"Predicted Tags:\", tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5P6fVthiGtk",
        "outputId": "5b98bcb2-260a-4ce8-fbdc-e86eafe2107e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Predicted Tags: [('android',)]\n"
          ]
        }
      ],
      "source": [
        "new_question = \"How would you implement a RecyclerView to display a dynamic list of items with multiple view types?\"\n",
        "tags = predict_tags(new_question)\n",
        "print(\"Predicted Tags:\", tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jXL9wG_jWq1",
        "outputId": "9ca5f092-6846-499a-ff35-79b1a76a6d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Predicted Tags: [('angularjs',)]\n"
          ]
        }
      ],
      "source": [
        "new_question = \"How would you create a custom directive in a framework to bind dynamic data and manipulate the DOM while maintaining modularity and reusability?\"\n",
        "tags = predict_tags(new_question)\n",
        "print(\"Predicted Tags:\", tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
